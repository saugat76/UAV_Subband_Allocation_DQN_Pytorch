diff --git a/contention_env_complex/__pycache__/misc.cpython-310.pyc b/contention_env_complex/__pycache__/misc.cpython-310.pyc
index 7d4e890..b624677 100644
Binary files a/contention_env_complex/__pycache__/misc.cpython-310.pyc and b/contention_env_complex/__pycache__/misc.cpython-310.pyc differ
diff --git a/contention_env_complex/__pycache__/uav_env.cpython-310.pyc b/contention_env_complex/__pycache__/uav_env.cpython-310.pyc
index 0cb57f4..29a19dc 100644
Binary files a/contention_env_complex/__pycache__/uav_env.cpython-310.pyc and b/contention_env_complex/__pycache__/uav_env.cpython-310.pyc differ
diff --git a/contention_env_complex/main.py b/contention_env_complex/main.py
index 373f340..713fd0c 100644
--- a/contention_env_complex/main.py
+++ b/contention_env_complex/main.py
@@ -73,8 +73,8 @@ def parse_args():
     parser.add_argument("--grid-space", type=int, default=100, help="seperating space for grid")
     parser.add_argument("--uav-dis-th", type=int, default=1000, help="distance value that defines which uav agent share info")
     parser.add_argument("--dist-pri-param", type=float, default=1/5, help="distance penalty priority parameter used in level 3 info exchange")
-    parser.add_argument("--coverage-threshold", type=int, default=75, help="if coverage threshold not satisfied, penalize reward, in percentage")
-    parser.add_argument("--coverage-penalty", type=int, default=10, help="penalty value if threshold is not satisfied")
+    parser.add_argument("--coverage-threshold", type=int, default=70, help="if coverage threshold not satisfied, penalize reward, in percentage")
+    parser.add_argument("--coverage-penalty", type=int, default=5, help="penalty value if threshold is not satisfied")
 
 
     args = parser.parse_args()
@@ -170,12 +170,13 @@ class DQL:
             next_state = next_state.to(device = device)
             done = done.to(device = device)
 
-            diff = state - next_state
-            done_local = (diff != 0).any(dim=1).float().to(device)
+            # diff = state - next_state
+            # done_local = (done != 0).any(dim=1).float().to(device)
 
             # Implementation of DQL algorithm 
             Q_next = self.target_network(next_state).detach()
-            target_Q = reward.squeeze() + self.gamma * Q_next.max(1)[0].view(batch_size, 1).squeeze() * done_local
+            # target_Q = reward.squeeze() + self.gamma * Q_next.max(1)[0].view(batch_size, 1).squeeze() * done_local
+            target_Q = reward.squeeze() + self.gamma * Q_next.max(1)[0].view(batch_size, 1).squeeze()
 
             # Forward 
             # Loss calculation based on loss function
@@ -343,6 +344,25 @@ if __name__ == "__main__":
             # Also calculting episodic reward for each agent // Add this in your main program 
             episode_reward_agent = np.add(episode_reward_agent, reward)
 
+            # Also calculting episodic reward for each agent // Add this in your main program 
+            episode_reward_agent = np.add(episode_reward_agent, reward)
+            
+            
+            ##########################
+            ####       Logs       ####
+            ##########################
+
+            for k in range(NUM_UAV):
+                if len(UAV_OB[k].replay_buffer) > batch_size:
+                    UAV_OB[k].train(batch_size, dnn_epoch, k)
+                    if args.wandb_track:
+                        wandb.log({f"loss__{k}" : UAV_OB[k].loss})
+
+            # Keeping track of covered users every time step to ensure the hard coded value is satisfied
+            writer.add_scalar("chart/covered_users_per_timestep", temp_data[6], (i_episode * num_epochs + t))
+            if args.wandb_track:
+                wandb.log({"covererd_users_per_timestep": temp_data[6], "timestep": (i_episode * num_epochs + t) })
+
             states = next_state
 
             for k in range(NUM_UAV):
diff --git a/contention_env_complex/uav_env.py b/contention_env_complex/uav_env.py
index 17a4706..5ebfb1a 100644
--- a/contention_env_complex/uav_env.py
+++ b/contention_env_complex/uav_env.py
@@ -284,7 +284,7 @@ class UAVenv(gym.Env):
             for k in range(self.NUM_UAV):
                 if self.flag[k] != 0:
                     reward_solo[k] = np.copy(sum_user_assoc[k] - 2) - penalty_overlap[k]
-                    isDone = True
+                    # isDone = True
                 else:
                     reward_solo[k] = (sum_user_assoc[k] - penalty_overlap[k])
                 if total_covered_users <= self.args.coverage_threshold:
@@ -307,7 +307,7 @@ class UAVenv(gym.Env):
                     temp_user_id = np.where(dist_uav_uav[k, :] <= self.UAV_DIST_THRS)
                     reward_ind[k] = np.average(sum_user_assoc_temp[temp_user_id])
                     reward_ind[k] -= 2
-                    isDone = True
+                    # isDone = True
                 else:
                     temp_user_id = np.where(dist_uav_uav[k, :] <= self.UAV_DIST_THRS)
                 if total_covered_users <= self.args.coverage_threshold:
@@ -315,7 +315,7 @@ class UAVenv(gym.Env):
                     reward_ind[k] = np.copy(reward_ind[k] - self.args.coverage_penalty)
             reward = np.copy(reward_ind)
 
-        
+        isDone = False
         # Defining the reward function by the number of covered user
         ################################################################
         ##            Opt.4  No. of User Covered as Reward            ##
